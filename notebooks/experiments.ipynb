{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5d11ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e53a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0fab4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f53fc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# model = ChatGroq(model=\"qwen/qwen3-32b\",temperature=0.7)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4ab01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "# \tresult = model.invoke(\"What is the capital of France?\").content\n",
    "# \tprint(result)\n",
    "# except Exception as e:\n",
    "# \tprint(f\"Error: {e}\")\n",
    "# \tprint(\"Please check your internet connection, API key, and Groq API status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0afac7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='mistral'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "ollama_chat_model = ChatOllama(model=\"mistral\")\n",
    "print(ollama_chat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac2f0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(ollama_chat_model.invoke(\"What is the capital of France?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aadf895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "print(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a632f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = embedding_model.embed_query('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e472d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2affc4",
   "metadata": {},
   "source": [
    "1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66d0d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cceb16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suel.Abbasi\\Desktop\\agentic ai and RAG\\Krish Nair\\document_portal\\notebooks\\data\\sample.pdf\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "file_path = Path(os.path.join(os.getcwd(), 'data', 'sample.pdf'))\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df0e484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8032915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2ac75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                    chunk_overlap=150,  # experimental value, there is not deterministic way\n",
    "                    length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d298c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "046d9bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "478eb1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Suel.Abbasi\\\\Desktop\\\\agentic ai and RAG\\\\Krish Nair\\\\document_portal\\\\notebooks\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 10, 'page_label': '11'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[100].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9affe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b422aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstores = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c8ab4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS is in memory, so it will not be saved to disk.\n",
    "# To save it to disk, you can use the `save_local` method.\n",
    "# on disk storage (faiss we cab persist voer the disk like we do in chroma)\n",
    "# not available on cloud (pinecone, azure  etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "757946a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c0499342-e695-4691-b563-d05bd7e8ffb4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Suel.Abbasi\\\\Desktop\\\\agentic ai and RAG\\\\Krish Nair\\\\document_portal\\\\notebooks\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77'),\n",
       " Document(id='3dee4528-cd02-41e6-8be0-1f5a7780d704', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Suel.Abbasi\\\\Desktop\\\\agentic ai and RAG\\\\Krish Nair\\\\document_portal\\\\notebooks\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='cd2d8cab-46b2-4bc3-9a28-2c61457fdbf4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Suel.Abbasi\\\\Desktop\\\\agentic ai and RAG\\\\Krish Nair\\\\document_portal\\\\notebooks\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 3, 'page_label': '4'}, page_content='7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\\nbut are not releasing.§\\n2. Llama 2-Chat, a fine-tuned version ofLlama 2that is optimized for dialogue use cases. We release\\nvariants of this model with 7B, 13B, and 70B parameters as well.\\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,'),\n",
       " Document(id='869291c3-0e5d-4ee8-91ec-7c253281bb69', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Suel.Abbasi\\\\Desktop\\\\agentic ai and RAG\\\\Krish Nair\\\\document_portal\\\\notebooks\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 33, 'page_label': '34'}, page_content='In addition, our study extended to evaluating theLlama 2-Chatwith access to a calculator. The results from\\nthis particular experiment are documented in Table 15. LLM tool use, while exciting, can also cause some\\nsafety concerns. We encourage more community research and red teaming in this area.\\n5.2 Limitations and Ethical Considerations\\nLlama 2-Chatis subject to the same well-recognized limitations of other LLMs, including a cessation of')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstores.similarity_search(\"What is the llama2 llm model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3146e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever =vectorstores.as_retriever()\n",
    "\n",
    "# create a chain \n",
    "# prompt template\n",
    "\n",
    "# context is somethin that we retireve from vectorstore\n",
    "# question is the user question\n",
    "prompt_template = \"\"\"You are a helpful assistant.\n",
    "You will be provided with a question and some context.\n",
    "Your task is to answer the question based on the context provided. IF context doesn't have the \n",
    "information, then say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "93a9c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} template='You are a helpful assistant.\\nYou will be provided with a question and some context.\\nYour task is to answer the question based on the context provided. IF context doesn\\'t have the \\ninformation, then say \"I don\\'t know\".\\n\\nContext: {context}\\nQuestion: {question}\\nAnswer:'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")   \n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60341066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f028a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'context': VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000157EA586A50>, search_kwargs={})\n",
       "  | RunnableLambda(format_docs),\n",
       "  'question': RunnablePassthrough()},\n",
       " PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful assistant.\\nYou will be provided with a question and some context.\\nYour task is to answer the question based on the context provided. IF context doesn\\'t have the \\ninformation, then say \"I don\\'t know\".\\n\\nContext: {context}\\nQuestion: {question}\\nAnswer:')\n",
       " | ChatOllama(model='mistral')\n",
       " | StrOutputParser())"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f5f0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass this prompt tp llm\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs , \"question\": RunnablePassthrough()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73092bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rag_chain | prompt | ollama_chat_model | output_parser\n",
    "response  = chain.invoke(\"tell me about the llama2 finetuning benchmark experiments within 25 words\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856f5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa626ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca2d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
